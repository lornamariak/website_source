[{"categories":["Productivity"],"contents":" One of my goals for the second half of the year is to help as many people to get a grip on their learning goals for 2019. Throughout my YouTube series Learning with Lorna, I have shared the first step to the learning process, making a learning plan.\nKey points about a learning plan  A learning plan is a document that lays out learning prospects over time. When one takes interest in a topic/skill and intends to start learning it, it is important to draft a plan that fits within their schedule to elaborate on how they will pursue this new-found interest. A learning plan is not a timetable. The plan takes in your learning goals and helps you map out the steps, actions and resources needed to meet them and the evidence that you need to measure success.   Section 1 Name: Fill in your name or the name of the person you’re preparing this for. Date: Fill in the intended learning period e.g 19th June 2019 to 19th July 2019\nSection 2 This section is a personal introspective centred on helping you discover what you want to meet from this learning process. Vision: What do you envision yourself achieving by learning this topic/skill? Expected outcomes: What impact do you want to make by learning this? (centre this on yourself)\nSection 3 Topics General topic: Fill in the major topics and break them down into manageable sub-topics, if possible break it down further to fill the sub-specific topics. This will help you have a precise definition of what you’re trying to learn.\n Learning Aids In order to maximise your learning goals, you need to take advantage of the resources available to help you learn better. These include tutorials, articles, podcasts, books etc. It is essential for you to map these out ahead of your learning journey and go ahead to insert references to specific resources if you have already found them. This saves a lot of time when you start your learning process and helps you focus more on learning than trying to find resources.\nTime frame Fill in the days and hours you’re dedicating to each sub topic: being specific to the hours will help you know how many hours you cumulatively dedicated to the learning which is a metric of accountability.\nPlace A great learning environment contributes to the learning process. Depending on what you’re learning and what type of learner you’re, you have to choose your environment carefully and prepare it in advance to avoid distractions and save time. Find out what type of learner you’re here.\nExpected Outcomes For a successful learning process, your goals should be measurable. In the outcomes, set simple actions you can fulfil after each topic learned. These can include taking a quiz, presenting to fellow learners among others. These will cumulatively be a way to measure the success of your learning process.\nNotes A space reserved for any comments about the process you have mapped out. You can use it for budgeting your learning process, evaluating how much you need to successfully set aside to buy resources, fund your movements etc. and compare with any available alternatives. The goal is to keep your learning process affordable.\n Use this link to download a learning plan template of the format described in this article. Feel free to give me feedback on your own use case of the template.I am happy to hear more people learning something new and challenging themselves further. Happy learning! 😻\nYoutube video   ","permalink":"/blog/learningplan/","tags":["Learning"],"title":"Ease your way into new topics with a learning plan."},{"categories":["Life Lessons"],"contents":" Photo Credit:Christina Morillo\nBackground When starting a community we usually have this long vision of seeing it evolve into a huge impactful part of the domain it belongs to. In my case, when I started running data ladies I focused majorly on developing the technical skills of women interested in data science but I soon realised that it grew into something consistent and I needed to scale into a full-blown data science community. Did I? my year review can answer this! Community building involves technical and non-technical elements. \nHere are some of the lessons I learnt. Core skills and codependency The focus on core/beginner skills in the data science field gave me a great starting advantage of nurturing participants from day one and later it became easy to introduce other much harder skills like machine learning. Every community has participants with different experience levels, focus on making these groups codependent.\nIt is great to have the novice group learn from the intermediate and advanced groups, the intermediate group is great at being learning facilitator for the novice group while working along the advanced and expert groups , the expert group is great at guiding and facilitating workshops for beginners and working alongside advanced group are great at coming up with learning material for the novice and intermediate groups. The expert group can always take up interns/volunteers from any group. This will be an automated mentorship model in your community.\nKnowing your community members When you run consistent meet-ups you will start to see a number of members that keep showing up, build relations with these people, talk to them and start to involve them in the community, sometimes the creativity you need to hire to do something will be just among your people so pay attention.\nThis will also make leadership an easy transition in the community. Most communities I have been part of never had democratically elected leaders, most leadership was through volunteering and giving back to the community. Knowing my community members I was always ready to find who can work best in which place.\nAll learning must be fun and safe No matter what topic or tool you decide to tackle, make the learning environment as fun as possible keeping in mind that community guidelines must be clear to everyone and followed religiously regardless of who they apply to. While communities are fun and free, they may also turn hostile if not governed by a certain level of rules.\nFunding: Partnership, sponsorships and scholarships. Most communities start while relying on external funding. It is very important to reach out to as many people or companies as possible and fine tune every partnership to also benefit the targeted sponsors. don’t make your proposals sound self-centred, be strategic. Funding doesn\u0026rsquo;t only come in terms of money, some people will give you space, a projector, snacks, photography, influencing, promo codes etc, take advantage of these to make your events great. Always give your sponsors and supporting people feedback and a thank you note.\nConsistency and Visibility It is important to keep up with a pattern of events and if possible make a calendar and share with the community in advance. Be sure to also share your work especially on the meet-up social media or blog/website and urge your participants to share their growth too. Growth will highly depend on feedback and follow-up and you can use a number of creative tools to collect feedback other that forms. Check out Kahoot and mentimeter. This will show you areas to improve, topics to rerun among other metrics. \nReflection Looking back at all these lessons I learned, I wished I had more time to spend with my data ladies group and even work hard to grow it further. However, I have noticed that two groups in the same niche have sprung up; R Ladies Kampala and Women in Machine learning and data science Kampala chapter. I hope this article enlightens one thing or two for anyone looking at starting a learning group/community online or offline.\n","permalink":"/blog/2019-11-27-an-organizer-s-lessons-on-meet-ups-and-growing-a-community/","tags":["Life Lessons"],"title":"An organizer's lessons on meet-ups and growing a community"},{"categories":null,"contents":"I am a data analyst and content creator. I split my time between building data science projects in and out of work and creating content on this blog and my Youtube channel. My content ranges from data science tutorials,productivity hacks and general life topics that interest the present day millenial.\nInterests are data, research on AI and bias, meaningful conversations and cross culture learning.\n","permalink":"/about/about/","tags":null,"title":"About Me"},{"categories":["Life Lessons"],"contents":" I don’t see a utopia anywhere for me. ~ Bozoma Saint John\nI cannot believe that 2018 has come to an end! Just like many years of my adult life, this one started off with lots of projections and goals. Today I want to take off time and share three aspects of my life that I felt can help someone reading this. \nCareer and Learning I started off this year at the entry level of my career. Although I had past working experiences around the building of developer communities, I had picked up a keen interest in data science and the year began with me deciding to have a full-time data science career. I used as many learning methods as possible to help me grasp skills that would put me on the job market real quick. Thanks to the R stats community, I realised that I had to learn and write. This made me more creative, gave me the zeal to learn more concepts and the feedback was so encouraging. This is by far an all-around method that not only helped me learn quickly but also helped me showcase my skills and that’s where my entry to the industry began.\nWhat is it like to be a data scientist in the industry? I often get this question a lot especially when people find out what I do versus my job title. I will mention that being a data scientist in the industry is sometimes not a mentioned task, sometimes no one at your workplace even calls you a data [insert all those cool titles] but it is your responsibility to show your team that data exists and can be incorporated in decision making. My past 3 months at my new workplace have taught me a million lessons about being a data scientist in the field that none of the blogs on the internet ever did. So if you ask what it is like to be in the industry, it is tough, more than python and R and all those fancy words in the tech and data industry, it is research, asking the right questions and formulating the right comparisons. Soon you realise that models are not the hardest thing but how to formulate the right models. \nClose and switch. Often times friends ask me, why don’t you do tech communities anymore, you should do one more event or which one is the next event/ community you’re organising? Well after close to four years of being a community organiser, I rediscovered myself. I must appreciate that it is through community organising that I discovered my true strengths and passions. I worked with promoting the use of different technologies, inclusion and diversity in tech and when 2018 came, I decided that I would go out and practice a part of me that I had discovered about two years ago. It is okay to quit that original thing that everyone thinks you should stick to and go after that thing that you think will help you make a difference in the world. I love to watch communities grow, mentor people into the whole culture but also not forgetting to work on my true self. I think the wave is beautiful to ride but don’t forget yourself in that ride. \nLife and relocating. If you had told me at the beginning of this year that I would not spend the most important holidays with my family I would have said, well unless I am on Mars. I made a bold decision to move into my own apartment because of numerous reasons but autonomy was top of the list. If you have grown up in a community where personal space is not really a thing you always wonder what it feels like. Well, self-reliance is as challenging as possible but every challenge is a lesson for years to come. During this year, I had a chance to relocate(bucket list ticked). I enjoyed fitting my entire life in complimentary checked in luggage and starting all over but one big lesson that I learnt from this is it is never too late to start learning again. Cultural shock, weather changes, language barrier are all things that can take a toll on you while relocating. Be aware and protect your sanity in that phase because things will go wrong and you will have to keep calm. But amidst all this I have really grown into a responsible human being, I am more aware of my surrounding, I plan and budget and I can account for everything I do. I manage my time and all this comes from constant self-training. \nTake time off and alone time. I used to think holiday/vacation was just a fancy thing but guess what, I found out that it is instead a healthy thing to do. For your health, sanity and well being if not for your muscles, take time off and switch off the work. I took two holidays this year, both were a week long and they gave me a great rest, peace of mind and the best two weeks of 2018. The activities of this year were taking a toll on me and everything was happening so fast, I was stressed, angry and feisty. One little thing would go wrong and I would lose my mind. The air around me was tense and negative but taking time off and crying, ranting and reading just helped transform this energy into something positive. Life is not all roses remember. Not only did I enjoy the beautiful sights, but I also rediscovered myself and set my priorities, I learnt how to read a map(not google maps), talked to strangers and took beautiful pictures. \nHello 2019 2019 is my year to finally confront my bigger fears. I look forward to taking a very bold career step that will probably leave a mark on me for the next ten years. I also plan to start worrying less about things I cannot control and not to mention taking the matters of my diet and exercise routine into my own hands. I keep promising myself a better body but this time I am more than willing to give myself a better body. Cheers, share with me your learning and life goals.Happy new 2019 to all of us!\n","permalink":"/post/2019-11-28-a-2018-review/","tags":["Life Lessons"],"title":"My 2018 Review;Learning with life"},{"categories":["Data Science"],"contents":" Published in The StartUp Publication\nIntroduction One of the applications of text mining is sentiment analysis.In order for us to go ahead and carry out a sentiment analysis of our mined text,we are required to clean and prepare our data set as we saw in a previous article.\n Understanding Sentiment Analysis Sentiment Analysis : The study of extracted information to identify reactions, attitudes, context and emotions.As one of the applications of text mining, sentiment analysis exposes the attitudes in the mined text. It is based on word polarities, it takes into account positive or negative words and neutral words are dismissed.\nTable showing word polarities\nSentiment analysis is done based on lexicons. A lexicon in simpler terms is a vocabulary, say the English lexicon.In this context, a lexicon is a selection of words with the two polarities that can be used as a metric in sentiment analysis. There are many different types of lexicons that can be used depending on the context of the data you are working with.There is also a possibility of creating a custom lexicon depending on how much customisation we would like to make with your data.\nIn this article,we shall make use of the [syuzhet]() package.While there are a number of packages for sentiment analysis on CRAN,the syuzhet package is great to learn with because it is a combination of the most common lexicons like nrc, bing and afinn. We also make use of ggplot2 to further visualise our results from the sentiment analysis.\n How does Sentiment analysis work? In simple terms,sentiment analysis is performed as an intersection of a term-document (built from the mined text) and a lexicon of choice.\nThe first step is to have a term-document and a lexicon of your choice. Then form an intersection between the two sets.  Hands-on with Sentiment analysis  Example one: This is a simple example where we extract emotions from a sentence.We load the sentence,split each word using the strsplit() function to form a character vector and use the get_nrc_sentiment() function from the syuzhet library.This function takes in new_sentence and compares it with the nrc emotion lexicon to return the scores as shown below.\nlibrary(syuzhet) sentence \u0026lt;- \u0026quot;i love cats such a bundle of joy.\u0026quot; new_sentence \u0026lt;- as.character(strsplit(sentence,\u0026quot; \u0026quot;)) get_nrc_sentiment(new_sentence) #This is the output anger anticipation disgust fear joy sadness surprise trust negative 0 0 0 0 2 0 0 0 0 positive 2  Example two: This second example makes use of a TED talks data set that was downloaded from Kaggle under the name transcript.csv.It underwent cleaning using the tm package following the steps in this article and was carried forward for sentiment analysis.\n#load the libraries library(syuzhet) library(tm) library(ggplot2) #mydataCopy is a term document,generated from cleaning #transcripts.csv mydataCopy \u0026lt;- mydata #carryout sentiment mining using the get_nrc_sentiment()function #log the findings under a variable result result \u0026lt;- get_nrc_sentiment(as.character(mydataCopy)) #change result from a list to a data frame and transpose it result1\u0026lt;-data.frame(t(result)) #rowSums computes column sums across rows for each level of a #grouping variable. new_result \u0026lt;- data.frame(rowSums(result1)) #name rows and columns of the dataframe names(new_result)[1] \u0026lt;- \u0026quot;count\u0026quot; new_result \u0026lt;- cbind(\u0026quot;sentiment\u0026quot; = rownames(new_result), new_result) rownames(new_result) \u0026lt;- NULL #plot the first 8 rows,the distinct emotions qplot(sentiment, data=new_result[1:8,], weight=count, geom=\u0026quot;bar\u0026quot;,fill=sentiment)+ggtitle(\u0026quot;TedTalk Sentiments\u0026quot;) #plot the last 2 rows ,positive and negative qplot(sentiment, data=new_result[9:10,], weight=count, geom=\u0026quot;bar\u0026quot;,fill=sentiment)+ggtitle(\u0026quot;TedTalk Sentiments\u0026quot;)   Plot1: Shows distinct emotions\nPlot2: shows a combination of emtions under 2 polarities\n Conclusion The code from this example can be accessed from this repository. We have applied our sentiment analysis tricks on mined text to come up with an evident description of the emotions attached to text data.This could be a whole project that can help you gain insights on how and when to talk to your audience, what they feel about a certain topic /product/service and what better way you can interact with them.\n","permalink":"/post/exploring-sentiment-analysis/","tags":["Rstats"],"title":"Exploring Sentiment Analysis"},{"categories":["Data Science"],"contents":" Published in Towards Data Science\nIntroduction One of the things that makes shiny apps interactive is reactivity. In the simplest of terms reactivity/reactive programming is the ability of a program to compute outputs from a given set of user inputs. The ability of a shiny app to handle reactivity makes a two-way communication between the user and the existing information. Reactivity is applied in cases such as performing calculations, data manipulation, the collection of user information among other scenarios. As a beginner setting out to build shiny apps, having the basic knowledge to handle reactivity will help you go a long way to exploring different use cases of shiny apps.\n Let’s get started The idea of reactivity will not occur to one until the error message below.\nThis error occurs when a reactive component is placed inside a non reactive function. The app will not load and will parse this error. Let’s us look at what a reactive function is and what it does.\nReactive Components of a shiny app There are three major reactive components of a shiny app:\nReactive Inputs: A reactive input is defined as an input that a user provides through the browser interface. For example when a user fills a form,selects an item or clicks a button. These actions will trigger values to be set form the reactive inputs.\nReactive Outputs: A reactive output is defined as program provided output in the browser interface. For example a graph, a map, a plot or a table of values.Table of values as a reactive output.\nReactive Expressions: A reactive expression is defined as one that transforms the reactive inputs to reactive outputs.These perform computations before sending reactive outputs.These will also mask slow operations like reading data from a server, making network calls among other scenarios.We shall see one in an example.\n Example 1 Let’s start with a simple example of adding up two integers and returning their sum in a shiny app.\nui titlePanel(\u0026quot;Sum of two integers\u0026quot;), #number input form sidebarLayout( sidebarPanel( textInput(\u0026quot;one\u0026quot;, \u0026quot;First Integer\u0026quot;), textInput(\u0026quot;two\u0026quot;, \u0026quot;Second Integer\u0026quot;), actionButton(\u0026quot;add\u0026quot;, \u0026quot;Add\u0026quot;) ), # Show result mainPanel( textOutput(\u0026quot;sum\u0026quot;) ) server server \u0026lt;- function(input,output,session) { #observe the add click and perform a reactive expression observeEvent( input$add,{ x \u0026lt;- as.numeric(input$one) y \u0026lt;- as.numeric(input$two) #reactive expression n \u0026lt;- x+y output$sum \u0026lt;- renderPrint(n) } )  Example 2 Now let’s build something a bit complex while handling reactivity.\nui fields \u0026lt;- c(\u0026quot;name\u0026quot;,\u0026quot;age\u0026quot;,\u0026quot;height\u0026quot;,\u0026quot;weight\u0026quot;) ui \u0026lt;- fluidPage( # Application title titlePanel(\u0026quot;Health card\u0026quot;), # Sidebar with reactive inputs sidebarLayout( sidebarPanel( textInput(\u0026quot;name\u0026quot;,\u0026quot;Your Name\u0026quot;), selectInput(\u0026quot;age\u0026quot;,\u0026quot;Age bracket\u0026quot;,c(\u0026quot;18-25\u0026quot;,\u0026quot;25-45\u0026quot;,\u0026quot;above 45\u0026quot;)), textInput(\u0026quot;weight\u0026quot;,\u0026quot;Please enter your weight in kg\u0026quot;), textInput(\u0026quot;height\u0026quot;,\u0026quot;Please enter your height in cm\u0026quot;), actionButton(\u0026quot;save\u0026quot;,\u0026quot;Add\u0026quot;) ), # a table of reactive outputs mainPanel( mainPanel( DT::dataTableOutput(\u0026quot;responses\u0026quot;, width = 500), tags$hr() ) ) ) ) server # Define server logic server \u0026lt;- function(input, output,session) { #create a data frame called responses saveData \u0026lt;- function(data) { data \u0026lt;- as.data.frame(t(data)) if (exists(\u0026quot;responses\u0026quot;)) { responses \u0026lt;\u0026lt;- rbind(responses, data) } else { responses \u0026lt;\u0026lt;- data } } loadData \u0026lt;- function() { if (exists(\u0026quot;responses\u0026quot;)) { responses } } # Whenever a field is filled, aggregate all form data #formData is a reactive function formData \u0026lt;- reactive({ data \u0026lt;- sapply(fields, function(x) input[[x]]) data }) # When the Save button is clicked, save the form data observeEvent(input$save, { saveData(formData()) }) # Show the previous responses # (update with current response when save is clicked) output$responses \u0026lt;- DT::renderDataTable({ input$save loadData() }) }  Result There you go! Now that you can handle the basics, please go ahead and try it out.\n","permalink":"/post/reactivity-in-shiny-library/","tags":["Rstats"],"title":"Reactivity in Shiny library"},{"categories":["Data Science"],"contents":" Published in Towards Data Science\nIntroduction Over the years, social media has become a hot spot for data mining. Every day there are always topics trending, campaigns running and groups of people discussing different global, continental or national issues.This is a major target to harness data. In this article, we focus on Twitter as a centre of opinions and sentiments across the globe.We set out to mine text from the millions of tweets that go out every day to be able to understand what is going on all even beyond our own timelines. \nTwitter API Set Up To use the twitter API we need to have a twitter account. Sign Up via (https://twitter.com) ,go to (https://apps.twitter.com/) to access twitter developer options. Create a new application\nApplication Name: Give your app a unique name. if it is taken you will be notified to change.\nApplication site: This can be a link to your Github repository where the application code will be if your app has no site yet. (like mine)\nCall back URL: This is a link to which a success or failure message is relayed from one program to another.It tells the program on where to go next in both cases.You can direct your application to any port available, mine is port 1410.\nApp Credentials These are very important to help one log into the application.There are four major credentials used in this set up.\nConsumer key : This key identifies the client to the application.\nConsumer Secret: This is the clients password that is used with server authentication.\nAccess Token:This is the consumer identification that is used to define their privileges.\nAccess Secret:This is sent with the access token as a password. They are obtained this way.\nNote:These credentials are meant to be kept private,that is why i shaded through mine. Voila! We have an API set up\n R Studio Set Up R uses the twitteR library, an R based Twitter client that handles communication with the Twitter API. Let us take a moment and thank Jeff Gentry for putting this library together.\n#from CRAN install.packages(“twitteR”) #alternatively from the Github library(devtools) install_github(“geoffjentry/twitteR”)  The difference between the two methods above is that the first method downloads the package from the CRAN site and will take the argument of a package name while the second will install packages from the GitHub repository and will take an argument of the repository name. Read more about packages and installation here.\nAuthentication Twitter uses Open Authentication (OAuth) to grant access to the information. Open Authentication is a token based authentication method.Let’s refer to our four credentials.\n#load library library(twitteR) #load credentials consumer_key \u0026lt;- “****************” consumer_secret\u0026lt;- “*******************” access_token \u0026lt;- “*******************” access_secret \u0026lt;- “************************” #set up to authenticate setup_twitter_oauth(consumer_key ,consumer_secret,access_token ,access_secret)  We use the setup_twitter_oauth function to set up our authentication.The setup_twitter_oauth() function takes in the four twitter credentials that we generated from the API set up above. Go ahead and authorise direct authentication by pressing Y on our keyboard.\nQuerying Twitter To query is to simply ask a question. To be able to access the data we need, we have to send meaningful queries to twitter. With Twitter, we have access to tons of information from trends to campaigns to accounts. We have a range of things to query. Let us run query on this hashtag #rstats\n#fetch tweets associated with that hashtag , 12 tweets-n in #(en)glish-lang since the indicated date yy/mm/dd tweets \u0026lt;- twitteR::searchTwitter(“#rstats”,n =12,lang =”en”,since = ‘2018–01–01’) #strip retweets strip_retweets(tweets)  This code returns the tweets as a list.The strip_retweets() function eliminates any retweets in the returned tweets. For further analyse these tweets we shall consider converting the returned tweets to a data frame and store them locally.\n#convert to data frame using the twListtoDF function df \u0026lt;- twListToDF(tweets)\\#extract the data frame save it locally saveRDS(df, file=”tweets.rds”) df1 \u0026lt;- readRDS(“mytweets.rds”)  Cleaning the Tweets From the query, we have managed to store the results into a data frame on our computers.Now let us examine this data to find out whoever has the highest Retweets and give them a shoutout straight from our script. We shall use the dplyr library to traverse through this data frame.\nlibrary(dplyr) #clean up any duplicate tweets from the data frame using #dplyr::distinct dplyr::distinct(df1)  Let us make use of dplyr verbs to select the tweet,screenname,id and retweet count for a tweet with the most retweets and store the result in a data frame called winner.\nwinner \u0026lt;-df1 %\u0026gt;% select(text,retweetCount,screenName,id )%\u0026gt;% filter(retweetCount == max(retweetCount)) View(winner)  Check out more on dplyr via this cheat sheet.\nSending a Direct Message To notify our retweet competition winner ,we shall send them a direct message from this script by picking their handle from the winner data frame. The dmsend() function takes in the message and username which is saved as the screenName.\nus \u0026lt;- userFactory$new(screenName= winner$screenName) dmSend(“Thank you for participating in #rstats,Your tweet had the highest retweets”, us$screenName)  Conclusion We have just created our first mined dataset from twitter and used it to find out our retweet competition winner. Well, that is just a small example of what we can do, however, there is a column that contains text from the mined tweets which can be analysed using techniques from the other articles on this blog. There are more possibilities with this mined data like sentiment analysis and word frequency visualisations. Happy Tweet Mining!\n","permalink":"/post/setting-up-twitter-for-text-mining-in-r/","tags":["Rstats"],"title":"Setting Up Twitter for Text mining in R"},{"categories":["Data Science"],"contents":" Published in Towards Data Science\nIntroduction One of the reasons data science has become popular is because of it’s ability to reveal so much information on large data sets in a split second or just a query. Think about it deeply ,on a daily basis how much information in form of text do we give out? All this information contains our sentiments,our opinions ,our plans ,pieces of advice ,our favourite phrase among other things. However revealing each of those this can seem like finding a needle from a haystack at a glance ,until we use techniques like text mining/analysis . Text mining takes in account information retrieval ,analysis and study of word frequencies and pattern recognition to aid visualisation and predictive analytics. In this article ,We go through the major steps that a data set undergoes to get ready for further analysis.we shall write our script using R and the code will be written in R studio. To achieve our goal ,we shall use an R package called “tm”.This package supports all text mining functions like loading data,cleaning data and building a term matrix.It is available on CRAN.\nLet’s install and load the package in our work space to begin with. #downloading and installing the package from CRAN install.packages(\u0026quot;tm\u0026quot;) #loading tm library(tm)   Loading Data Text to be mined can be loaded into R from different source formats.It can come from text files(.txt),pdfs (.pdf),csv files(.csv) e.t.c ,but no matter the source format ,to be used in the tm package it is turned into a corpus. A corpus is defined as “a collection of written texts, especially the entire works of a particular author or a body of writing on a particular subject”. The tm package use the Corpus() function to create a corpus.\n#loading a text file from local computer newdata\u0026lt;- readlines(filepath) #Load data as corpus #VectorSource() creates character vectors mydata \u0026lt;- Corpus(VectorSource(newdata))  Refer to this guide to learn more about importing files into R.\n Cleaning Data. Once we have successfully loaded the data into the work space,it is time to clean this data. Our goal at this step is to create independent terms(words) from the data file before we can start counting how frequent they appear. Since R is case sensitive ,we shall first convert the entire text to lowercase to avoid considering same words like “write” and “Write” differently. We shall remove : URLs ,emojis,non-english words,punctuations,numbers,whitespace and stop words.\nStop words : The commonly used english words like “a”,” is ”,”the” in the tm package are referred to as stop words. These words have to be eliminated so as to render the results more accurate.It is also possible to create your own custom stop words.\n# convert to lower case mydata \u0026lt;- tm_map(mydata, content_transformer(tolower)) #remove ������ what would be emojis mydata\u0026lt;-tm_map(mydata, content_transformer(gsub), pattern=\u0026quot;\\\\W\u0026quot;,replace=\u0026quot; \u0026quot;) # remove URLs removeURL \u0026lt;- function(x) gsub(\u0026quot;http[^[:space:]]*\u0026quot;, \u0026quot;\u0026quot;, x) mydata \u0026lt;- tm_map(mydata, content_transformer(removeURL) ) # remove anything other than English letters or space removeNumPunct \u0026lt;- function(x) gsub(\u0026quot;[^[:alpha:][:space:]]*\u0026quot;, \u0026quot;\u0026quot;, x) mydata \u0026lt;- tm_map(mydata, content_transformer(removeNumPunct)) # remove stopwords mydata \u0026lt;- tm_map(mydata, removeWords, stopwords(\u0026quot;english\u0026quot;)) #u can create custom stop words using the code below. #myStopwords \u0026lt;- c(setdiff(stopwords('english'), c(\u0026quot;r\u0026quot;, \u0026quot;big\u0026quot;)),\u0026quot;use\u0026quot;, \u0026quot;see\u0026quot;, \u0026quot;used\u0026quot;, \u0026quot;via\u0026quot;, \u0026quot;amp\u0026quot;) #mydata \u0026lt;- tm_map(mydata, removeWords, myStopwords) # remove extra whitespace mydata \u0026lt;- tm_map(mydata, stripWhitespace) # Remove numbers mydata \u0026lt;- tm_map(mydata, removeNumbers) # Remove punctuations mydata \u0026lt;- tm_map(mydata, removePunctuation)   Stemming Stemming is the process of gathering words of similar origin into one word for example “communication”, “communicates”, “communicate”. Stemming helps us increase accuracy in our mined text by removing suffixes and reducing words to their basic forms.We shall use the SnowballC library.\nlibrary(SnowballC) mydata \u0026lt;- tm_map(mydata, stemDocument)  Building a term Matrix and Revealing word frequencies After the cleaning process ,we are left with independent terms that exist throughout the document.These are stored in a matrix that shows each of their occurrence. This matrix logs the number of times the term appears in our clean data set thus being called a term matrix.\n#create a term matrix and store it as dtm dtm \u0026lt;- TermDocumentMatrix(mydata)  Word frequencies :These are the number of times words appear in data set.Word frequencies will indicate to us from the most frequently used words in the data set to the least used using the compilation of occurrences from the term matrix.\n Conclusion We have just written a basic text mining script ,however it is just the beginning of text mining.The ability to get the text in its raw format and clean it to this point will give us direction to things like building a word cloud,sentiment analysis and building models.\n","permalink":"/post/text-mining-script-with-r/","tags":["Rstats"],"title":"Writing your first Text Mining Script with R"}]